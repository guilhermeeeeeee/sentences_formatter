Um algoritmo é um esquema de resolução de um problema. Pode ser implementado com qualquer sequência de valores ou objetos que tenham uma lógica infinita (por exemplo, a língua portuguesa, a linguagem Pascal, a linguagem C, uma sequência numérica, um conjunto de objetos tais como lápis e borracha), ou seja, qualquer coisa que possa fornecer uma sequência lógica. 
Embora tenhamos que usar um algoritmo prévio na nossa língua para escrever um programa com lógica, o próprio programa que provém desse algoritmo já é um algoritmo. Até um esquema mental é um algoritmo.
Ok, já percebi o que é um algoritmo. Mas porque isso interessa ao estudo da programação?
A verdade é que, antes de escrevermos um programa em qualquer outra linguagem é necessário escrever um esquema em papel para evitar erros, por exemplo, na nossa língua, segundo o programa que queremos fazer. Com isto não esquecemos a lógica que queremos dar ao programa e será menos comum o aparecimento de erros. Por exemplo:
Linguagem humana:
"Se for verdade isso, acontece isto, senão acontece aquilo"
Linguagem de máquina:
IF isso; THEN isto; ELSE aquilo;
O conteúdo escrito acima está formalizado numa linguagem de algoritmos chamada Portugol que é a representação utilizada pela maior parte dos programadores e professores que trabalham em instituições de ensino de linguagens de programação.
Como pode visualizar, um algoritmo pode ser escrito de várias maneiras, de cima para baixo, da esquerda para a direita, na diagonal, em árabe, em russo... O importante é que seja escrito!
Uma máquina computacional é qualquer máquina (geralmente de origem eletro-eletrônica) com capacidade de receber dados, executar operações sobre estes dados e retornar os dados transformados por estas operações.
Entrada de Dados	Processamento	Saída de Dados
As máquinas computacionais eletro-eletrônicas possuem geralmente dois componentes básicos: software e hardware. Chamamos de Hardware sua parte física, e software os programas que tratam os dados imputados.
Quando inserimos algum dado em um computador, os dados inseridos são transformados em sinais elétricos (chamados de bits). O bit (do inglês binary digit) representa os dois estados (ligado ou desligado) que o sinal elétrico pode assumir. Para trabalhar com estes dados, podemos associar estes estados de ligado e desligado a 0 e 1. Quando utilizamos um computador, há um fluxo de sinais elétricos, que representam os dados inseridos, processados e retornados. Um conjunto de oito bits formam um byte, que é uma unidade completa de informação.

Dentro do byte, o estado de cada um dos oito bits, assim como sua posição relativa um ao outro, faz com que o byte assuma um valor específico (não necessariamente numérico), que serve para estruturá-lo em relação a outros bytes e criar um sistema de dados que sirva ao usuário externo.

Para organizar as possibilidades de variações destes bits dentro de um byte
Logicamente torna-se trabalhoso trabalhar com dados de computador bit-a-bit. Como forma de manipular este fluxo de estados elétricos e estruturá-lo de forma a permitir operações mais simplificadas e otimizadas sobre os bytes, surgiu o conceito de programação. As linguagens de programação são geralmente em dois níveis:

Linguagens de Baixo Nível: são linguagens de programação que tratam a informação na linguagem de máquina.
Linguagens de Alto Nível: são linguagens de programação modeladas quase como a linguagem comum humana, que quando compiladas são convertidas para linguagem de máquina. Cada linguagem deste tipo possui uma sintaxe própria, que deve ser respeitada e aprendida para que possa ser corretamente processada por seu compilador. Compilador é um programa que permite que determinada programação em uma linguagem específica seja adaptada para linguagem de máquina.
No entanto, não é necessário que o programador aprenda todas as diversas linguagens disponíveis no mercado. Cada linguagem é recomendada para determinadas aplicações, assim como possuem suas sintaxes próprias, mas todas são estruturadas logicamente. Com o aprendizado da Lógica de Programação o aluno entenderá os conceitos básicos da programação e poderá com menor ou maior dificuldade, dependendo da linguagem escolhida, aprender a linguagem que quiser.

As linguagens de programação tratam os dados de um computador através do uso de algoritmos. Um algoritmo é uma estruturação passo-a-passo de como um determinado problema deve ser resolvido de forma não-ambígua (ou como muitos comparam "uma receita de bolo") . Desta forma, para realizar esta estruturação é necessário o uso de ferramentas e operações oriundas da Lógica, principalmente da Lógica Matemática.

Antes de estruturar-se de forma lógica para programação, devemos saber qual o tipo de problema proposto, as informações que serão imputadas e os passos a serem efetuados para atingir-se um fim específico. 
A capacidade de pensar de maneira lógica é um dos principais diferenciais para saber como resolver problemas, principalmente na área da computação. Diretamente relacionado a isso, compreender o conceito de algoritmo também é algo fundamental. Com isso em mente, caso seu objetivo seja se tornar um bom programador, o próximo passo é aprender um pseudocódigo (Portugol, por exemplo), no qual você entrará em contato com detalhes como entrada e saída de dados, assim como o processamento propriamente dito. Nesse artigo, veremos um pouco sobre esses conceitos, fundamentais para quem está começando a programar.
A lógica de programação é a capacidade que todo programador precisa ter para resolver os problemas que aparecem no dia-a-dia. A capacidade de dividir o problema em partes menores é uma etapa essencial da lógica de programação e precisa ser levada em consideração quando nos deparamos com qualquer exercício/desafio. É nesse ponto que entra o conceito de algoritmo, descrito, geralmente, como uma sequência lógica de ações capaz de resolver um problema.

É válido ressaltar, no entanto, que o conceito de algoritmo vai muito além da programação. Uma receita de bolo, por exemplo, é um exemplo simples de algoritmo. 

A maioria dos algoritmos, obviamente, será mais complexa do que simplesmente “mascar um chiclete” ou “fazer um bolo”. No caso da computação, uma soma de dois números é o exemplo mais clássico de um algoritmo simples. Algo como o mostrado na Listagem 2 é bastante comum para observarmos que temos uma sequência lógica de ações que envolvem os três elementos que comentamos anteriormente: entrada de dados, processamento e saída de dados.


A representação de algoritmos pode ser feita de várias formas; a que mostramos na Listagem 2 é uma delas. Entretanto, essa opção não possui nenhum tipo de regra mais formal, o que faz com que esteja um pouco distante da programação propriamente dita. Para aproximar um pouco mais os dois conceitos, o pseudocódigo foi criado (um dos exemplos de pseudocódigo mais conhecidos no Brasil é o Portugol). Além dele, podemos encontrar várias opções. O objetivo de todos, no entanto, é um só: criar um código que se aproxime do que será visto em linguagens de programação como C, C# ou Java.

Utilizando o Portugol como exemplo, nosso algoritmo da soma de dois números seria algo como o mostrado na Listagem 3. Nessa “linguagem de programação”, é importante observar que temos um outro conceito, conhecido como regiões; nesse código, temos uma região de variáveis e outra de processamento. Esse conceito também é algo que será levado para as linguagens de programação e sempre deve ser considerado durante o desenvolvimento de software.

Com isso, podemos observar que o Portugol possui regras simples e bastante flexíveis. Ao começar a programar com Java ou C#, por exemplo, você irá notar que a proposta de se aproximar de linguagens como elas é obedecida. Se compararmos o código da Listagem 3 com um em C#, por exemplo, notaremos que não existe uma grande diferença. É claro que as ações (ler e escrever) e os operadores (atribuição e operador aritmético “+”) serão substituídos por seus respectivos na linguagem, mas a estrutura será semelhante à apresentada no exemplo.

Embora as vezes não percebemos, utilizamos algoritmos no nosso dia-a-dia e não sabemos. Para a execução de alguma tarefa ou mesmo resolver algum problema, muitas vezes inconscientemente executamos algoritmos. Mas o que é Algoritmo?

Algoritmo é simplesmente uma "receita" para executarmos uma tarefa ou resolver algum problema. E como toda receita, um algoritmo também deve ser finito. Se seguirmos uma receita de bolo corretamente, conseguiremos fazer o bolo. A computação utiliza muito esse recurso, então se você pretende aprender programação, obviamente deve saber o que é algoritmo.

Imagine o trabalho de um recepcionista de cinema, ele deve conferir os bilhetes e direcionar o cliente para a sala correta. Além disso, se o cliente estiver 30 minutos adiantado o recepcionista deve informar que a sala do filme ainda não está aberta. E quando o cliente estiver 30 minutos atrasado o recepcionista deve informar que a entrada não é mais permitida

Obs: Essas regras não são 100% verdade, eu as defini neste post apenas para fins didáticos

Vamos escrever um algoritmo para descrever a atividade do recepcionista.

Qualquer pessoa que seguir esses passos executará a função do recepcionista do cinema. Concorda? É importante notar que o algoritmo tem um fluxo que pode seguir diferentes caminhos dependendo da situação em que se encontra. Outro aspecto interessante é que o algoritmo é finito, uma hora ele tem que acabar! 
Esta representação gráfica do algoritmo é chamada de fluxograma. Os losangos representam as decisões que são tomadas para executar um ou outro passo. Ao final, a lâmpada tem que estar funcionando.

Todas as tarefas executadas pelo computador, são baseadas em Algoritmos. Logo, um algoritmo deve também ser bem definido, pois é uma máquina que o executará. Uma calculadora por exemplo, para executar a operação de multiplicação, executa um algoritmo que calcula somas até um determinado número de vezes. Abaixo, um exemplo do algoritmo de multiplicação. Para facilitar, consideremos que os fatores da multiplicação são positivos.

Este algoritmo pode ser considerado complexo por iniciantes, mas algoritmos deste tipo, utilizando variáveis e controle de fluxo, é muito comum em programação. Se você quer aprender programação, é necessário entendê-lo, se não conseguiu, leia-o novamente com mais atenção. Para ajudar, vamos definir algumas coisas importantes sobre o algoritmo.

Com o tempo, a leitura e criação de algoritmos passa a ser uma coisa muito simples para um programador. Mas para isso é preciso bastante prática! Então, você pode começar fazendo um exercício, crie algoritmos para as suas tarefas do dia-a-dia a partir do momento em que você acorda. Essa é a melhor forma de aprender a criar algoritmos.

A automação é o processo em que uma tarefa deixa de ser desempenhada pelo
homem e passa a ser realizada por máquinas, sejam estes dispositivos mecânicos, eletrônicos
(como os computadores) ou de natureza mista.
Para que a automação de uma tarefa seja bem-sucedida é necessário que a máquina
que passará a realizá-la seja capaz de desempenhar cada uma das etapas constituintes do
processo a ser automatizado com eficiência, de modo a garantir a repetibilidade do mesmo.
Assim, é necessário que seja especificado com clareza e exatidão o que deve ser realizado em
cada uma das fases do processo a ser automatizado, bem como a seqüência em que estas
fases devem ser realizadas.
À especificação da seqüência ordenada de passos que deve ser seguida para a
realização de uma tarefa, garantindo a sua repetibilidade, dá-se o nome de algoritmo.
Ao contrário do que se pode pensar, o conceito de algoritmo não foi criado para
satisfazer às necessidades da computação. Pelo contrário, a programação de computadores é
apenas um dos campos de aplicação dos algoritmos. Na verdade, há inúmeros casos que
podem exemplificar o uso (involuntário ou não) de algoritmos para a padronização do exercício
de tarefas rotineiras (vide exemplos da Seção 2.1). No entanto, daqui adiante a atenção desta
apostila estará voltada à automação de tarefas utilizando computadores.
Para que um computador possa desempenhar uma tarefa é necessário que esta seja
detalhada passo-a-passo, numa forma compreensível pela máquina, utilizando aquilo que se
chama de programa. Neste sentido, um programa de computador nada mais é que um
algoritmo escrito numa forma compreensível pelo computador (linguagem de programação). 

Serve como modelo para programas, pois sua linguagem é intermediária à linguagem
humana e às linguagens de programação, sendo então, uma boa ferramenta na validação da
lógica de tarefas a serem automatizadas.

Os algoritmos, servem para representar a solução de qualquer problema, mas no caso
do Processamento de Dados, eles devem seguir as regras básicas de programação para que
sejam compatíveis com as linguagens de programação.

Existem diversas formas de representação de algoritmos, mas não há um consenso
com relação à melhor delas.
O critério usado para classificar hierarquicamente estas formas está diretamente ligado
ao nível de detalhe ou, inversamente, ao grau de abstração oferecido.
Algumas formas de representação de algoritmos tratam os problemas apenas em nível
lógico, abstraindo-se de detalhes de implementação muitas vezes relacionados com alguma
linguagem de programação específica. Por outro lado existem formas de representação de
algoritmos que possuem uma maior riqueza de detalhes e muitas vezes acabam por
obscurecer as idéias principais do algoritmo, dificultando seu entendimento. 

Fluxograma é uma representação gráfica de algoritmos onde formas geométricas diferentes
implicam ações (instruções, comandos) distintos. Tal propriedade facilita o entendimento das
idéias contidas nos algoritmos e justifica sua popularidade.
Esta forma é aproximadamente intermediária à descrição narrativa e ao pseudocódigo
(subitem seguinte), pois é menos imprecisa que a primeira e, no entanto, não se preocupa com
detalhes de implementação do programa, como o tipo das variáveis usadas.
Nota-se que os fluxogramas convencionais preocupam-se com detalhes de nível físico
da implementação do algoritmo. Por exemplo, figuras geométricas diferentes são adotadas
para representar operações de saída de dados realizadas em dispositivos distintos, como uma
fita magnética ou um monitor de vídeo. Como esta apostila não está interessada em detalhes
físicos da implementação, mas tão somente com o nível lógico das instruções do algoritmo,
será adotada a notação simplificada da Figura 2.1 para os fluxogramas. De qualquer modo, o
Apêndice A contém uma tabela com os símbolos mais comuns nos fluxogramas convencionais. 

De modo geral, um fluxograma se resume a um único símbolo inicial por onde a
execução do algoritmo começa, e um ou mais símbolos finais, que são pontos onde a
execução do algoritmo se encerra. Partindo do símbolo inicial, há sempre um único caminho
orientado a ser seguido, representando a existência de uma única seqüência de execução das
instruções. Isto pode ser melhor visualizado pelo fato de que, apesar de vários caminhos
poderem convergir para uma mesma figura do diagrama, há sempre um único caminho saindo
desta. Exceções a esta regra são os símbolos finais, dos quais não há nenhum fluxo saindo, e
os símbolos de decisão, de onde pode haver mais de um caminho de saída (usualmente dois
caminhos), representando uma bifurcação no fluxo. 

Esta forma de representação de algoritmos é rica em detalhes, como a definição dos
tipos das variáveis usadas no algoritmo. Por assemelhar-se bastante à forma em que os
programas são escritos, encontra muita aceitação.
Na verdade, esta representação é suficientemente geral para permitir a tradução de um
algoritmo nela representado para uma linguagem de programação específica seja praticamente
direta. 

Há diversas formas de representação de algoritmos que diferem entre si pela
quantidade de detalhes de implementação que fornecem ou, inversamente, pelo grau de
abstração que possibilitam com relação à implementação do algoritmo em termos de uma
linguagem de programação específica.
Dentre as principais formas de representação de algoritmos destacam-se: a descrição
narrativa, o fluxograma convencional e o pseudocódigo (ou linguagem estruturada). 

Todo o trabalho realizado por um computador é baseado na manipulação das
informações contidas em sua memória. Grosso modo, estas informações podem ser
classificadas em dois tipos

A maior parte das pessoas não ligadas à área de informática ignora o potencial dos
computadores e imagina que eles são capazes de tratar apenas com dados numéricos. Na
realidade, a capacidade dos mesmos se estende a outros tipos de dados.
O objetivo deste capítulo é justamente o de classificar os dados de acordo com o tipo
de informação contida neles. A classificação apresentada não se aplica a nenhuma linguagem
de programação específica; pelo contrário, ela sintetiza os padrões utilizados na maioria das
linguagens.

Os números inteiros são aqueles que não possuem componentes decimais ou
fracionários, podendo ser positivos ou negativos.
Os elementos pertencentes aos conjuntos N e Z, apesar de serem representáveis na
classe dos números reais, são classificados como dados do tipo inteiro, por não possuírem
parte fracionária. Esta possibilidade é interessante por permitir uma economia do espaço de
memória, como veremos adiante.
Por sua vez, os elementos dos conjuntos Q e R, por possuírem parte fracionária, não
podem ser representados na classe inteira, pertencendo necessariamente aos tipos de dados
ditos reais. 
Os dados de tipo real são aqueles que podem possuir componentes decimais ou
fracionários, e podem também ser positivos ou negativos.
Como dito anteriormente, os elementos dos conjuntos de números fracionários e reais
são necessariamente representados no computador por dados do tipo real.
O tipo de dados literal é constituído por uma seqüência de caracteres contendo letras,
dígitos e/ou símbolos especiais. Este tipo de dados é também muitas vezes chamado de
alfanumérico, cadeia (ou cordão) de caracteres, ainda, do inglês, string. 
Usualmente, os dados literais são representados nos algoritmos pela coleção de
caracteres, delimitada em seu início e término com o caractere aspas (").
Diz-se que o dado do tipo literal possui um comprimento dado pelo número de
caracteres nele contido
A existência deste tipo de dado é, de certo modo, um reflexo da maneira como os
computadores funcionam. Muitas vezes, estes tipos de dados são chamados de booleanos,
devido à significativa contribuição de BOOLE à área da lógica matemática.
O tipo de dados lógico é usado para representar dois únicos valores lógicos possíveis:
verdadeiro e falso. É comum encontrar-se em outras referências outros tipos de pares de
valores lógicos como sim/não, 1/0, true/false.
Nos algoritmos apresentados nesta apostila os valores lógicos serão delimitados pelo
caractere ponto (.). 
A todo momento durante a execução de qualquer tipo de programa os computadores
estão manipulando informações representadas pelos diferentes tipos de dados descritos no
capítulo anterior. Para que não se "esqueça" das informações, o computador precisa guardálas em sua memória.
Este capítulo é destinado ao estudo da forma como os computadores armazenam e
acessam informações contidas em sua memória
Cada um dos diversos tipos de dados apresentados no capítulo anterior necessita de
uma certa quantidade de memória para armazenar a informação representada por eles.
Esta quantidade é função do tipo de dado considerado, do tipo da máquina
(computador) e do tipo de linguagem de programação. Por isso, o que será exposto nos
subitens seguintes não deve ser tomado como padrão, apenas como exemplo. 
Devemos sempre ter em mente que um byte consegue representar 256 (28
)
possibilidades diferentes.
Uma informação do tipo literal nada mais é do que um conjunto de caracteres que
podem ser letras, dígitos ou símbolos especiais.
A união de todos os caracteres existentes nos computadores resulta num conjunto com
um número de elementos menor que 256. Deste resultado surgiu a idéia de associar a cada
caractere um número (código) variando de 0 a 255 (256 possibilidades). No princípio, cada
fabricante de computador adotava uma convenção diferente para este código. Mais
recentemente, esta convenção foi padronizada a fim de facilitar a portabilidade (migração) de
programas entre máquinas diferentes. Esta convenção é representada na forma de uma tabela
de mapeamento de caracteres em números. O padrão mais universalmente aceito é o ASCII,
cuja tabela é mostrada no Apêndice B.
Assim, cada célula de memória (byte) pode conter um caractere, representado pelo seu
código ASCII.
Retornando à questão do armazenamento de informações do tipo literal na memória,
deve-se lembrar que um dado deste tipo possui um certo comprimento dado pelo número de
caracteres nele contido. Portanto, para guardar um dado do tipo literal devemos alocar
(reservar) um espaço contíguo de memória igual ao comprimento do mesmo, destinando um
byte para cada caractere da informação.
Exemplificando, a informação do tipo literal "banana" possui seis caracteres e, portanto,
seis bytes são necessários para reter a referida informação na memória. A princípio, estes
bytes podem estar em qualquer lugar da memória, mas é conveniente que estejam juntos
(posições contíguas). A primeira posição deste conjunto de bytes é absolutamente arbitrária e
sua escolha geralmente é feita automaticamente pelo compilador (isto é, pelo programa que traduz um outro escrito em alguma linguagem de programação para outra geral, a linguagem
de máquina do computador com que se trabalha). 
Uma informação do tipo lógico só possui dois valores possíveis. Assim, a
princípio, um único bit seria suficiente para armazenar uma informação deste tipo. Contudo,
deve-se lembrar que a menor porção de memória que se pode acessar é o byte. Portanto, uma
informação do tipo lógico é armazenada em um byte de memória. De certa forma, se por um
lado isto pode ser como um "desperdício" de memória, por outro simplifica bastante a
arquitetura de memória dos computadores (por motivos que fogem ao contexto desta apostila).
Além do mais, isto não é tão relevante, uma vez que na prática o número de ocorrências de
dados do tipo lógico é bastante inferior ao de ocorrências de dados do tipo literal ou numérico. 
O conjunto dos números reais (R) contém um número infinito de elementos e, pelas
mesmas razões que o conjunto dos números inteiros, precisa ser limitado.
Para dados deste tipo julgou-se apropriado adotar quatro bytes para sua representação
interna nos computadores.
São muito comuns situações como as aplicações científicas em que é necessária uma
maior precisão de cálculo, intimamente ligada ao número de casas decimais dos dados. Para
este caso, em analogia com o que acontece com os dados do tipo inteiro, algumas linguagens
de programação decidiram criar dados do tipo real estendido (com oito bytes).
Como visto anteriormente, informações correspondentes a diversos tipos de dados são
armazenadas na memória dos computadores. Para acessar individualmente cada uma destas
informações, a princípio, seria necessário saber o tipo de dado desta informação (ou seja, o
número de bytes de memória por ela ocupados) e a posição inicial deste conjunto de bytes na
memória.
Percebe-se que esta sistemática de acesso a informações na memória é bastante
ilegível e difícil de se trabalhar. Para contornar esta situação criou-se o conceito de variável,
que é uma entidade destinada a guardar uma informação.
Basicamente, uma variável possui três atributos: um nome, um tipo de dado
associado à mesma e a informação por ela guardada. 
Toda variável possui um nome que tem a função de diferenciá-la das demais. Cada
linguagem de programação estabelece suas próprias regras de formação de nomes de
variáveis.
Todas as variáveis utilizadas em algoritmos devem ser definidas antes de serem
utilizadas. Isto se faz necessário para permitir que o compilador reserve um espaço na
memória para as mesmas.
a palavra-chave VAR deverá estar presente sempre e será utilizada uma única vez na
definição de um conjunto de uma ou mais variáveis; numa mesma linha poderão ser definidas uma ou mais variáveis do mesmo tipo. Para
tal, deve-se separar os nomes das mesmas por vírgulas. 
variáveis de tipos diferentes devem ser declaradas em linhas diferentes. 
A forma de utilização deste comando ficará mais clara quando da utilização da
representação de algoritmos em linguagem estruturada (pseudocódigo).
Esta convenção é válida para a representação de algoritmos na forma de pseudocódigo.
Em termos de fluxograma, não é usual adotar-se qualquer forma de definição de variáveis. 
A memória dos computadores é composta por células numeradas ordenadamente
denominadas bytes. Cada byte é constituído por 8 bits.
Cada tipo de dado requer um número diferente de bytes para armazenar a informação
representada por ele na memória. Esta quantidade também pode variar em função do tipo de
computador considerado.
Uma variável é uma entidade dotada de um nome para diferenciá-la das demais e um
tipo de dado que define o tipo de informação que ela é capaz de guardar. Uma vez definidos,
o nome e o tipo de uma variável não podem ser alterados no decorrer de um programa. Por
outro lado, a informação útil da variável é objeto de constante modificação durante o decorrer
do programa, de acordo com o fluxo de execução do mesmo. 
O conceito de expressão em termos computacionais está intimamente ligado ao
conceito de expressão (ou fórmula) matemática, onde um conjunto de variáveis e constantes
numéricas relacionam-se por meio de operadores aritméticos compondo uma fórmula que, uma
vez avaliada, resulta num valor. 
A lógica da programação é um assunto muito grande e complexo. Assim sendo, vamos dar
apenas uma introdução a história da lógica da programação.A lógica de programação é quando
se pretende realizar alguma função ou um esquema lógico por meio de parâmetros e metas.
Existe uma associação direta da Lógica de Programação com o Raciocínio Matemático, onde o
importante é a interpretação de um problema e a utilização correta de uma fórmula. De
fato, não existe “fórmulas” em informática, o que existe é nosso modo de pensar 
em como resolver e extrair o máximo de informações de um problema, de maneira eficaz
e eficiente sobre um ângulo de visão. Essa solução precisa ser exteriorizada e expressa numa
linguagem conhecida. A lógica da programação entra nesse ponto, para desenvolvermos soluções e algorítimos para
apresentar essa solução ao mundo. A primeira pessoa a pensar em usar lógica matemática para
a programação foi John McCarthy, ele propôs usar programas para manipular com sentenças instrumentais comuns
apropriadas à linguagem formal, ou seja, o programa básico formará conclusões imediatas a partir de
uma lista de premissas. Essas conclusões serão tanto sentenças declarativas quanto imperativas. Quando uma sentença 
imperativa é deduzida, o programa toma uma ação correspondente. 






A lógica da programação é um assunto muito grande e complexo. Assim sendo, vamos dar
apenas uma introdução a história da lógica da programação.A lógica de programação é quando
se pretende realizar alguma função ou um esquema lógico por meio de parâmetros e metas.
Existe uma associação direta da Lógica de Programação com o Raciocínio Matemático, onde o
importante é a interpretação de um problema e a utilização correta de uma fórmula. De
fato, não existe “fórmulas” em informática, o que existe é nosso modo de pensar 
em como resolver e extrair o máximo de informações de um problema, de maneira eficaz
e eficiente sobre um ângulo de visão. Essa solução precisa ser exteriorizada e expressa numa
linguagem conhecida. A lógica da programação entra nesse ponto, para desenvolvermos soluções e algorítimos para
apresentar essa solução ao mundo. A primeira pessoa a pensar em usar lógica matemática para
a programação foi John McCarthy, ele propôs usar programas para manipular com sentenças instrumentais comuns
apropriadas à linguagem formal, ou seja, o programa básico formará conclusões imediatas a partir de
uma lista de premissas. Essas conclusões serão tanto sentenças declarativas quanto imperativas. Quando uma sentença 
imperativa é deduzida, o programa toma uma ação correspondente.
